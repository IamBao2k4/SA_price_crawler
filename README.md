# Binance Crawler - Clean Architecture

Real-time cryptocurrency data pipeline using Kafka + MongoDB with Clean Architecture.

## ğŸ—ï¸ Architecture

```
Binance API â†’ Producer â†’ Kafka â†’ Consumer â†’ MongoDB
              (Crawler)         (Storage)
```

**Tech Stack:**
- **Data Source:** Binance API
- **Message Queue:** Apache Kafka
- **Database:** MongoDB
- **Language:** Python 3.11+
- **Architecture:** Clean Architecture (SOLID principles)

## ğŸš€ Quick Start

### Option 1: Docker (Recommended)

```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f crawler consumer

# Monitor via Kafka UI
open http://localhost:8080
```

### Option 2: Local Development

```bash
# Install dependencies
pip install -r requirements.txt

# Start Kafka & MongoDB (Docker)
docker-compose up -d kafka mongodb

# Run producer
python main_producer.py

# Run consumer (in another terminal)
python main_consumer.py
```

## ğŸ“ Project Structure

```
binance_crawler/
â”œâ”€â”€ src/                    # Source code (Clean Architecture)
â”‚   â”œâ”€â”€ config/            # Configuration management
â”‚   â”œâ”€â”€ core/              # Domain models & interfaces
â”‚   â”œâ”€â”€ producers/         # Data sources (Binance)
â”‚   â”œâ”€â”€ storage/           # Infrastructure (Kafka, MongoDB)
â”‚   â”œâ”€â”€ consumers/         # Application services
â”‚   â””â”€â”€ utils/             # Utilities
â”‚
â”œâ”€â”€ main_producer.py       # Entry point: Producer
â”œâ”€â”€ main_consumer.py       # Entry point: Consumer
â”‚
â”œâ”€â”€ docker-compose.yml     # Docker orchestration
â”œâ”€â”€ Dockerfile             # Container image
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ symbols_top20.txt      # Trading symbols list
â”‚
â”œâ”€â”€ ARCHITECTURE.md        # Architecture documentation
â”œâ”€â”€ DOCKER_GUIDE.md        # Docker detailed guide
â”œâ”€â”€ DOCKER_QUICKSTART.md   # Docker quick start
â”œâ”€â”€ REFACTORING.md         # Migration from old code
â”‚
â””â”€â”€ legacy/                # Old code (deprecated)
```

## âš™ï¸ Configuration

Configure via environment variables:

```bash
# Kafka
export KAFKA_BOOTSTRAP_SERVERS=kafka:9092
export KAFKA_TOPIC_PREFIX=binance.klines

# MongoDB
export MONGODB_URI=mongodb://admin:admin123@mongodb:27017/
export MONGODB_DATABASE=binance

# Crawler
export ACTIVE_INTERVALS=1m,5m,15m,1h,4h,1d
export LOG_LEVEL=INFO
```

Or use `.env` file (see `.env.example`).

## ğŸ“Š Features

### Producer (Crawler)
âœ… Crawl multiple intervals (1m, 5m, 15m, 1h, 4h, 1d, etc.)
âœ… Real-time data streaming to Kafka
âœ… Smart update frequency (no missing candles)
âœ… Automatic retry on errors
âœ… Rate limit protection

### Consumer (Storage)
âœ… Batch processing for efficiency
âœ… Upsert to MongoDB (no duplicates)
âœ… Automatic indexing
âœ… Statistics tracking

### Infrastructure
âœ… Kafka for reliable messaging
âœ… MongoDB for persistent storage
âœ… Docker for easy deployment
âœ… Kafka UI for monitoring

## ğŸ¯ Use Cases

- **Real-time Trading Bots:** Get latest candle data < 15s latency
- **Backtesting:** Query historical data from MongoDB
- **Analytics:** Process data from Kafka stream
- **Multiple Consumers:** Add more consumers for different purposes

## ğŸ“š Documentation

- **[ARCHITECTURE.md](ARCHITECTURE.md)** - Clean Architecture details
- **[DOCKER_QUICKSTART.md](DOCKER_QUICKSTART.md)** - Docker quick start
- **[DOCKER_GUIDE.md](DOCKER_GUIDE.md)** - Docker detailed guide
- **[REFACTORING.md](REFACTORING.md)** - Migration from old code

## ğŸ§ª Testing

```bash
# Test producer
python main_producer.py

# Test consumer
python main_consumer.py

# Check MongoDB
docker exec -it binance_mongodb mongosh -u admin -p admin123
> use binance
> db.klines.countDocuments()

# Check Kafka UI
open http://localhost:8080
```

## ğŸ“ˆ Monitoring

### Kafka UI
- URL: http://localhost:8080
- View topics, messages, consumer lag

### MongoDB
```bash
# Connect
docker exec -it binance_mongodb mongosh -u admin -p admin123

# Query
use binance
db.klines.countDocuments()
db.klines.find({symbol: "BTCUSDT", interval: "1h"}).sort({open_time: -1}).limit(5)
```

### Logs
```bash
# Docker logs
docker-compose logs -f crawler
docker-compose logs -f consumer

# Local logs
./logs/
```

## ğŸ”§ Development

### Add New Data Source

Implement `IDataSource` interface:

```python
# src/producers/coinbase.py
from src.core.interfaces import IDataSource

class CoinbaseDataSource(IDataSource):
    def fetch_klines(self, symbol, interval, limit):
        # Implement Coinbase API
        pass
```

### Add New Storage

Implement `IStorage` interface:

```python
# src/storage/postgres_storage.py
from src.core.interfaces import IStorage

class PostgresStorage(IStorage):
    def save_batch(self, klines):
        # Implement Postgres logic
        pass
```

See [ARCHITECTURE.md](ARCHITECTURE.md) for more details.

## ğŸ› Troubleshooting

### Producer not publishing?
```bash
# Check Kafka is running
docker-compose ps kafka

# Check logs
docker-compose logs crawler
```

### Consumer not consuming?
```bash
# Check consumer group
docker exec binance_kafka kafka-consumer-groups \
  --bootstrap-server localhost:9092 \
  --describe --group binance-consumer-group
```

### MongoDB connection error?
```bash
# Check MongoDB
docker-compose ps mongodb
docker-compose logs mongodb
```

See [DOCKER_GUIDE.md](DOCKER_GUIDE.md) for more troubleshooting.

## ğŸ“¦ Dependencies

```
requests>=2.31.0        # HTTP client
pandas>=2.2.0           # Data processing
kafka-python>=2.0.2     # Kafka client
pymongo>=4.6.0          # MongoDB client
schedule>=1.2.0         # Task scheduling
```

## ğŸ“ Learn More

- **Clean Architecture:** See [ARCHITECTURE.md](ARCHITECTURE.md)
- **Kafka Basics:** See [DOCKER_GUIDE.md](DOCKER_GUIDE.md)
- **MongoDB Queries:** MongoDB documentation

## ğŸ“ License

MIT License - Feel free to use for your projects.

## ğŸ¤ Contributing

1. Fork the repo
2. Create feature branch
3. Follow Clean Architecture principles
4. Submit PR

## ğŸ“ Support

- Issues: GitHub Issues
- Questions: GitHub Discussions

---

**Built with Clean Architecture ğŸ›ï¸ | Powered by Kafka + MongoDB âš¡**
